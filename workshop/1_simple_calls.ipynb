{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "451495c7",
   "metadata": {},
   "source": [
    "# How to call a model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2ed79e",
   "metadata": {},
   "source": [
    "## Basic API calls using HTTP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64715f46",
   "metadata": {},
   "source": [
    "Here's a basic example of how to call an Ollama model using Python's `requests` library. \n",
    "\n",
    "Ollama runs locally on your machine and provides a simple HTTP API that doesn't require any API keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeba045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_API = \"http://ollama:11434/api/chat\"\n",
    "MODEL = \"ollama:qwen2.5:1.5b\"\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72121c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's response:\n",
      "\n",
      "Of course! Here's one for you:\n",
      "\n",
      "Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything!\n",
      "\n",
      "--------------------------------\n",
      "\n",
      "Full response:\n",
      "{\n",
      "  \"model\": \"qwen2.5:1.5b\",\n",
      "  \"created_at\": \"2026-01-20T11:45:27.422635856Z\",\n",
      "  \"message\": {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"Of course! Here's one for you:\\n\\nWhy don't scientists trust atoms?\\n\\nBecause they make up everything!\"\n",
      "  },\n",
      "  \"done\": true,\n",
      "  \"done_reason\": \"stop\",\n",
      "  \"total_duration\": 4209238579,\n",
      "  \"load_duration\": 2540307699,\n",
      "  \"prompt_eval_count\": 28,\n",
      "  \"prompt_eval_duration\": 601479011,\n",
      "  \"eval_count\": 23,\n",
      "  \"eval_duration\": 984805443\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "data = {\n",
    "    \"model\": \"qwen2.5:1.5b\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello! Can you tell me a joke?\"},\n",
    "    ],\n",
    "    \"stream\": False,\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = requests.post(url=OLLAMA_API, headers=headers, data=json.dumps(data))\n",
    "    response.raise_for_status()\n",
    "\n",
    "    response_data = response.json()\n",
    "\n",
    "    content = response_data[\"message\"][\"content\"]\n",
    "    print(f\"Model's response:\\n\\n{content}\", end=\"\\n\\n\")\n",
    "\n",
    "    print(\"-\" * 32)\n",
    "    print(\"\\nFull response:\")\n",
    "    print(json.dumps(response_data, indent=2))\n",
    "\n",
    "except requests.exceptions.ConnectionError as e:\n",
    "    print(\"\\nMake sure Ollama is running\\n\\n\")\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9818f9",
   "metadata": {},
   "source": [
    "# Pydantic AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c42d27b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of course! Here's one for you:\n",
      "\n",
      "Why did the tomato turn red?\n",
      "\n",
      "Because it saw the salad dressing!\n",
      "\n",
      "Is there anything else I can help you with, or do you want to hear another joke?\n"
     ]
    }
   ],
   "source": [
    "from pydantic_ai import Agent\n",
    "\n",
    "agent = Agent(\n",
    "    model=MODEL,\n",
    "    system_prompt=\"You are a helpful assistant.\",\n",
    ")\n",
    "\n",
    "result = await agent.run(\"Hello! Can you tell me a joke?\")\n",
    "\n",
    "print(result.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4569abda",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¯ Task 1: Customize the System Prompt\n",
    "\n",
    "**Objective**: Modify the system prompt to make the assistant respond as a pirate.\n",
    "\n",
    "**Instructions**: \n",
    "- Fill in the `system_prompt` variable below with a prompt that instructs the model to talk like a pirate\n",
    "- Run the cell to see your pirate assistant in action!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b895cbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Netherlands were the champions of the 1998 World Cup. They swept the tournament, beating France to win gold on home soil! It's like, imagine if all your favorite cartoons and movies happened in real lifeâ€”watching a Dutch team win a big sporting event would be right up your pirate side.\n"
     ]
    }
   ],
   "source": [
    "from pydantic_ai import Agent\n",
    "\n",
    "# TODO: Write a system prompt that makes the assistant talk like a pirate\n",
    "system_prompt = \"Talk like a pirate\"\n",
    "\n",
    "agent = Agent(\n",
    "    model=MODEL,\n",
    "    system_prompt=system_prompt,\n",
    ")\n",
    "\n",
    "result = await agent.run(\"Who won 1998 world cup?\")\n",
    "\n",
    "print(result.output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
