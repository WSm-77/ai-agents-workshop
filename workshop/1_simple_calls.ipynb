{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "451495c7",
   "metadata": {},
   "source": [
    "# How to call a model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2ed79e",
   "metadata": {},
   "source": [
    "## Basic API calls using HTTP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64715f46",
   "metadata": {},
   "source": [
    "Here's a basic example of how to call an Ollama model using Python's `requests` library. \n",
    "\n",
    "Ollama runs locally on your machine and provides a simple HTTP API that doesn't require any API keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeba045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_API = \"http://ollama:11434/api/chat\"\n",
    "MODEL = \"ollama:qwen2.5:1.5b\"\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72121c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's response:\n",
      "\n",
      "Here's one:\n",
      "\n",
      "What do you call a fake noodle?\n",
      "\n",
      "(Wait for it...)\n",
      "\n",
      "An impasta!\n",
      "\n",
      "Hope that made you smile! Do you want to hear another one?\n",
      "\n",
      "--------------------------------\n",
      "\n",
      "Full response:\n",
      "{\n",
      "  \"model\": \"llama3.2\",\n",
      "  \"created_at\": \"2026-01-14T17:59:19.815246658Z\",\n",
      "  \"message\": {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"Here's one:\\n\\nWhat do you call a fake noodle?\\n\\n(Wait for it...)\\n\\nAn impasta!\\n\\nHope that made you smile! Do you want to hear another one?\"\n",
      "  },\n",
      "  \"done\": true,\n",
      "  \"done_reason\": \"stop\",\n",
      "  \"total_duration\": 9315350184,\n",
      "  \"load_duration\": 4545648867,\n",
      "  \"prompt_eval_count\": 40,\n",
      "  \"prompt_eval_duration\": 1374951715,\n",
      "  \"eval_count\": 37,\n",
      "  \"eval_duration\": 3287104800\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "data = {\n",
    "    \"model\": \"qwen2.5:1.5b\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello! Can you tell me a joke?\"},\n",
    "    ],\n",
    "    \"stream\": False,\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = requests.post(url=OLLAMA_API, headers=headers, data=json.dumps(data))\n",
    "    response.raise_for_status()\n",
    "\n",
    "    response_data = response.json()\n",
    "\n",
    "    content = response_data[\"message\"][\"content\"]\n",
    "    print(f\"Model's response:\\n\\n{content}\", end=\"\\n\\n\")\n",
    "\n",
    "    print(\"-\" * 32)\n",
    "    print(\"\\nFull response:\")\n",
    "    print(json.dumps(response_data, indent=2))\n",
    "\n",
    "except requests.exceptions.ConnectionError as e:\n",
    "    print(\"\\nMake sure Ollama is running\\n\\n\")\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9818f9",
   "metadata": {},
   "source": [
    "# Pydantic AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c42d27b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "What do you call a fake noodle?\n",
      "\n",
      "(Wait for it...)\n",
      "\n",
      "An impasta!\n",
      "\n",
      "I hope that made you smile! Do you want to hear another one?\n"
     ]
    }
   ],
   "source": [
    "from pydantic_ai import Agent\n",
    "\n",
    "agent = Agent(\n",
    "    model=MODEL,\n",
    "    system_prompt=\"You are a helpful assistant.\",\n",
    ")\n",
    "\n",
    "result = await agent.run(\"Hello! Can you tell me a joke?\")\n",
    "\n",
    "print(result.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4569abda",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¯ Task 1: Customize the System Prompt\n",
    "\n",
    "**Objective**: Modify the system prompt to make the assistant respond as a pirate.\n",
    "\n",
    "**Instructions**: \n",
    "- Fill in the `system_prompt` variable below with a prompt that instructs the model to talk like a pirate\n",
    "- Run the cell to see your pirate assistant in action!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b895cbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrr, ye landlubbers! Yer lookin' fer the winner o' that there 1998 World Cup, eh? Alright then, matey... it were France that took home the silver treasure (or should I say, the FIFA World Trophy?) back in '98! They defeated Brazil 3-0 in the final. A grand victory fer les bleus, savvy?\n"
     ]
    }
   ],
   "source": [
    "from pydantic_ai import Agent\n",
    "\n",
    "# TODO: Write a system prompt that makes the assistant talk like a pirate\n",
    "system_prompt = \"Talk like a pirate\"\n",
    "\n",
    "agent = Agent(\n",
    "    model=MODEL,\n",
    "    system_prompt=system_prompt,\n",
    ")\n",
    "\n",
    "result = await agent.run(\"Who won 1998 world cup?\")\n",
    "\n",
    "print(result.output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
